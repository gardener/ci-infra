apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-prow-rules
  namespace: prow-monitoring
  labels:
    prometheus: prow
    role: alert-rules
spec:
  groups:
  - name: ci-absent
    rules:
    - alert: crierDown
      annotations:
        message: '@test-infra-oncall The service crier has been down for 10 minutes.'
      expr: |
        absent(up{job="crier"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: crier
    - alert: deckDown
      annotations:
        message: '@test-infra-oncall The service deck has been down for 10 minutes.'
      expr: |
        absent(up{job="deck"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: deck
    - alert: ghproxyDown
      annotations:
        message: '@test-infra-oncall The service ghproxy has been down for 10 minutes.'
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: ghproxy
    - alert: hookDown
      annotations:
        message: '@test-infra-oncall The service hook has been down for 10 minutes.'
      expr: |
        absent(up{job="hook"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: hook
    - alert: horologiumDown
      annotations:
        message: '@test-infra-oncall The service horologium has been down for 10 minutes.'
      expr: |
        absent(up{job="horologium"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: horologium
    - alert: prow-controller-managerDown
      annotations:
        message: '@test-infra-oncall The service prow-controller-manager has been down for 10 minutes.'
      expr: |
        absent(up{job="prow-controller-manager"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: prow-controller-manager
    - alert: sinkerDown
      annotations:
        message: '@test-infra-oncall The service sinker has been down for 10 minutes.'
      expr: |
        absent(up{job="sinker"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: sinker
    - alert: tideDown
      annotations:
        message: '@test-infra-oncall The service tide has been down for 10 minutes.'
      expr: |
        absent(up{job="tide"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: tide
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace {{ $labels.namespace }} is expected to fill up within a week. Currently {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[24h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: high
  - name: ghproxy
    rules:
    - alert: ghproxy-specific-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410",status=~"4..",path!="/repos/:owner/:repo/pulls/:pullId/requested_reviewers",path!="/search/issues",path!="/repos/:owner/:repo/pulls/:pullId/merge",path!="/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-422
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="422", status=~"4..",path=~"/repos/:owner/:repo/pulls/:pullId/requested_reviewers|/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-403
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="403", status=~"4..",path="/search/issues"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-405
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="405", status=~"4..",path="/repos/:owner/:repo/pulls/:pullId/merge"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"4..",status!="404",status!="410",status!="403",status!="405",status!="422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-403-405-422
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.k8s.io/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"403|405|422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: token {{ $labels.token_hash }} will run out of API quota before the next reset.
      expr: |
        github_token_usage{job="ghproxy"} <  1500
        and
        predict_linear(github_token_usage{job="ghproxy"}[30m], 1 * 3600) < 0
      for: 5m
      labels:
        severity: high
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 10m
      expr: |
        (sum(increase(prow_webhook_counter[1m])) == 0 or absent(prow_webhook_counter))
        and ((day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 16))
      for: 10m
      labels:
        severity: high
        slo: hook
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
  - name: prow-stale
    rules:
    - alert: Prow images are stale
      annotations:
        message: The prow images are older than 2 days for 24h.
      expr: |
        ((time()-max(prow_version) > 2 * 24 * 3600) and (day_of_week()<6) and (day_of_week()>2))
        or ((time()-max(prow_version) > 4 * 24 * 3600) and (day_of_week()==1))
        or ((time()-max(prow_version) > 4 * 24 * 3600) and (day_of_week()==2))
      for: 24h
      labels:
        severity: critical
  - name: Tide progress
    rules:
    - alert: Sync controller heartbeat
      annotations:
        message: The Tide "sync" controller has not synced in 15 minutes. See the <https://monitoring.prow.k8s.io/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&panelId=7|processing time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="sync"}[15m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: Status-update controller heartbeat
      annotations:
        message: The Tide "status-update" controller has not synced in 30 minutes. See the <https://monitoring.prow.k8s.io/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&panelId=7|processing time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="status-update"}[30m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: 'TidePool error rate: individual'
      annotations:
        message: At least one Tide pool encountered 3+ sync errors in a 10 minute window. If the TidePoolErrorRateMultiple alert has not fired this is likely an isolated configuration issue. See the <https://prow.k8s.io/tide-history|/tide-history> page and the <https://monitoring.prow.k8s.io/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&panelId=6&from=now-24h&to=now|sync error graph>.
      expr: |
        ((sum(increase(tidepoolerrors{org!="kubeflow"}[10m])) by (org, repo, branch)) or vector(0)) >= 3
      for: 5m
      labels:
        severity: warning
    - alert: 'TidePool error rate: multiple'
      annotations:
        message: Tide encountered 3+ sync errors in a 10 minute window in at least 3 different repos that it handles. See the <https://prow.k8s.io/tide-history|/tide-history> page and the <https://monitoring.prow.k8s.io/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&panelId=6&from=now-24h&to=now|sync error graph>.
      expr: |
        (count(sum(increase(tidepoolerrors[10m])) by (org, repo) >= 3) or vector(0)) >= 3
      for: 5m
      labels:
        severity: critical
        slo: tide
  - name: Blackbox Prober
    rules:
    - alert: 'Site unavailable: https://prow.k8s.io'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://prow.k8s.io|https://prow.k8s.io>.'
      expr: |
        min(probe_success{instance="https://prow.k8s.io"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: deck
    - alert: 'Site unavailable: https://monitoring.prow.k8s.io'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://monitoring.prow.k8s.io|https://monitoring.prow.k8s.io>.'
      expr: |
        min(probe_success{instance="https://monitoring.prow.k8s.io"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: monitoring
    - alert: 'Site unavailable: https://testgrid.k8s.io'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://testgrid.k8s.io|https://testgrid.k8s.io>.'
      expr: |
        min(probe_success{instance="https://testgrid.k8s.io"}) == 0
      for: 2m
      labels:
        severity: critical
    - alert: 'Site unavailable: https://gubernator.k8s.io'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://gubernator.k8s.io|https://gubernator.k8s.io>.'
      expr: |
        min(probe_success{instance="https://gubernator.k8s.io"}) == 0
      for: 2m
      labels:
        severity: critical
    - alert: 'Site unavailable: https://gubernator.k8s.io/pr/fejta'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://gubernator.k8s.io/pr/fejta|https://gubernator.k8s.io/pr/fejta>.'
      expr: |
        min(probe_success{instance="https://gubernator.k8s.io/pr/fejta"}) == 0
      for: 2m
      labels:
        severity: critical
    - alert: 'Site unavailable: https://storage.googleapis.com/k8s-gubernator/triage/index.html'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://storage.googleapis.com/k8s-gubernator/triage/index.html|https://storage.googleapis.com/k8s-gubernator/triage/index.html>.'
      expr: |
        min(probe_success{instance="https://storage.googleapis.com/k8s-gubernator/triage/index.html"}) == 0
      for: 2m
      labels:
        severity: critical
    - alert: 'Site unavailable: https://storage.googleapis.com/test-infra-oncall/oncall.html'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://storage.googleapis.com/test-infra-oncall/oncall.html|https://storage.googleapis.com/test-infra-oncall/oncall.html>.'
      expr: |
        min(probe_success{instance="https://storage.googleapis.com/test-infra-oncall/oncall.html"}) == 0
      for: 2m
      labels:
        severity: critical
  - name: Boskos resource usage
    rules:
    - alert: Resource exhausted (0% free)
      annotations:
        message: The Boskos resource "{{ $labels.type }}" has been exhausted (currently 0% free). See the <https://monitoring.prow.k8s.io/d/wSrfvNxWz/boskos-resource-usage?orgId=1|Boskos resource usage dashboard>.
      expr: |
        (
          min(boskos_resources{state="free"}) by (type, instance)
          and
          (sum(boskos_resources) by (type, instance) > 5)
        ) == 0
      labels:
        boskos_type: '{{ $labels.type }}'
        severity: high
  - name: Heartbeat ProwJobs
    rules:
    - alert: 'No recent successful runs: `ci-test-infra-prow-checkconfig`'
      annotations:
        message: '@test-infra-oncall The heartbeat job `ci-test-infra-prow-checkconfig` has not had a successful run in the past 20m (should run every 5m).'
      expr: |
        sum(increase(prowjob_state_transitions{job_name="ci-test-infra-prow-checkconfig", state="success"}[20m])) < 0.5
      labels:
        severity: critical
        slo: plank
  - interval: 1m
    name: SLO Compliance
    rules:
    - expr: |
        min((absent(ALERTS{alertstate="firing", slo="deck"}) or absent(ALERTS{alertstate="firing", slo="hook"}) or absent(ALERTS{alertstate="firing", slo="prow-controller-manager"}) or absent(ALERTS{alertstate="firing", slo="sinker"}) or absent(ALERTS{alertstate="firing", slo="tide"}) or absent(ALERTS{alertstate="firing", slo="monitoring"})) or (ALERTS{alertstate="firing", slo=~"deck|hook|prow-controller-manager|sinker|tide|monitoring"} - 1)) without (alertstate)
      record: slo_component_ok
    - expr: (vector(1) unless min(slo_component_ok == 0)) or (slo_component_ok == 0)
      record: slo_prow_ok
  - name: prow
    rules:
    - alert: prow-pod-crashlooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container}}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
      expr: rate(kube_pod_container_status_restarts_total{namespace=~"default|prow-monitoring",job="kube-state-metrics"}[5m]) * 60 * 5 > 0
      for: 1m
      labels:
        severity: critical
  - name: external-secret-sync
    rules:
    - alert: Failed-syncing-external-secret
      annotations:
        message: ExternalSecret {{ $labels.namespace }}/{{ $labels.name }} failed to be synced. does kubernetes-external-secrets-sa@k8s-prow.iam.gserviceaccount.com have `roles/secretmanager.viewer` and `roles/secretmanager.secretAccessor` permissions on the google secret manager secret used for this cluster secret?
      expr: |
        increase(kubernetes_external_secrets_sync_calls_count{job="kubernetes-external-secrets",status!="success"}[1m]) > 1.5
      labels:
        severity: critical
