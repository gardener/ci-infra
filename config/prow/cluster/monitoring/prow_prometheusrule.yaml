apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-prow-rules
  namespace: prow-monitoring
  labels:
    prometheus: prow
    role: alert-rules
spec:
  groups:
  - name: ci-absent
    rules:
    - alert: crierDown
      annotations:
        message: 'The service crier has been down for 10 minutes.'
      expr: |
        absent(up{job="crier"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: crier
    - alert: deckDown
      annotations:
        message: 'The service deck has been down for 10 minutes.'
      expr: |
        absent(up{job="deck"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: deck
    - alert: ghproxyDown
      annotations:
        message: 'The service ghproxy has been down for 10 minutes.'
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: ghproxy
    - alert: hookDown
      annotations:
        message: 'The service hook has been down for 10 minutes.'
      expr: |
        absent(up{job="hook"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: hook
    - alert: horologiumDown
      annotations:
        message: 'The service horologium has been down for 10 minutes.'
      expr: |
        absent(up{job="horologium"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: horologium
    - alert: prow-controller-managerDown
      annotations:
        message: 'The service prow-controller-manager has been down for 10 minutes.'
      expr: |
        absent(up{job="prow-controller-manager"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: prow-controller-manager
    - alert: sinkerDown
      annotations:
        message: 'The service sinker has been down for 10 minutes.'
      expr: |
        absent(up{job="sinker"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: sinker
    - alert: tideDown
      annotations:
        message: 'The service tide has been down for 10 minutes.'
      expr: |
        absent(up{job="tide"} == 1)
      for: 10m
      labels:
        severity: critical
        slo: tide
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace {{ $labels.namespace }} is expected to fill up within a week. Currently {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[24h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: high
  - name: ghproxy
    rules:
    - alert: ghproxy-specific-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410",status=~"4..",path!="/repos/:owner/:repo/pulls/:pullId/requested_reviewers",path!="/search/issues",path!="/repos/:owner/:repo/pulls/:pullId/merge",path!="/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-422
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="422", status=~"4..",path=~"/repos/:owner/:repo/pulls/:pullId/requested_reviewers|/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-403
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="403", status=~"4..",path="/search/issues"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-405
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are erroring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|the ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="405", status=~"4..",path="/repos/:owner/:repo/pulls/:pullId/merge"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"4..",status!="404",status!="410",status!="403",status!="405",status!="422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-403-405-422
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.prow.gardener.cloud/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=8|the ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"403|405|422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: token {{ $labels.token_hash }} will run out of API quota before the next reset.
      expr: |
        github_token_usage{job="ghproxy", resource!="search"} <  1500
        and
        predict_linear(github_token_usage{job="ghproxy", resource!="search"}[30m], 1 * 3600) < 0
      for: 5m
      labels:
        severity: high
  - name: abnormal webhook behaviors
    rules:
    # TODO: reconsider whether this alert makes sense at the scale of our org and with what timeframe based on some experience
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 1h
      expr: |
        (sum(increase(prow_webhook_counter[10m])) == 0 or absent(prow_webhook_counter))
        and ((day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 8) and (hour() <= 18))
      for: 1h
      labels:
        severity: high
        slo: hook
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
#  - name: prow-stale
#    rules:
#    - alert: Prow images are stale
#      annotations:
#        message: The prow images are older than 2 days for 24h.
#      expr: |
#        ((time()-max(prow_version) > 2 * 24 * 3600) and (day_of_week()<6) and (day_of_week()>2))
#        or ((time()-max(prow_version) > 4 * 24 * 3600) and (day_of_week()==1))
#        or ((time()-max(prow_version) > 4 * 24 * 3600) and (day_of_week()==2))
#      for: 24h
#      labels:
#        severity: critical
  - name: Tide progress
    rules:
    - alert: Sync controller heartbeat
      annotations:
        message: The Tide "sync" controller has not synced in 15 minutes. See the <https://monitoring.prow.gardener.cloud/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&viewPanel=7|processing time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="sync"}[15m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: Status-update controller heartbeat
      annotations:
        message: The Tide "status-update" controller has not synced in 30 minutes. See the <https://monitoring.prow.gardener.cloud/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&viewPanel=7|processing time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="status-update"}[30m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: 'TidePool error rate: individual'
      annotations:
        message: At least one Tide pool encountered 3+ sync errors in a 10 minute window. If the TidePoolErrorRateMultiple alert has not fired this is likely an isolated configuration issue. See the <https://prow.gardener.cloud/tide-history|/tide-history> page and the <https://monitoring.prow.gardener.cloud/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&viewPanel=6&from=now-24h&to=now|sync error graph>.
      expr: |
        ((sum(increase(tidepoolerrors{org!="kubeflow"}[10m])) by (org, repo, branch)) or vector(0)) >= 3
      for: 5m
      labels:
        severity: warning
    - alert: 'TidePool error rate: multiple'
      annotations:
        message: Tide encountered 3+ sync errors in a 10 minute window in at least 3 different repos that it handles. See the <https://prow.gardener.cloud/tide-history|/tide-history> page and the <https://monitoring.prow.gardener.cloud/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&viewPanel=6&from=now-24h&to=now|sync error graph>.
      expr: |
        (count(sum(increase(tidepoolerrors[10m])) by (org, repo) >= 3) or vector(0)) >= 3
      for: 5m
      labels:
        severity: critical
        slo: tide
  - name: Blackbox Prober
    rules:
    - alert: 'Site unavailable: https://prow.gardener.cloud'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://prow.gardener.cloud|https://prow.gardener.cloud>.'
      expr: |
        min(probe_success{instance="https://prow.gardener.cloud"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: deck
    - alert: 'Site unavailable: https://monitoring.prow.gardener.cloud'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://monitoring.prow.gardener.cloud|https://monitoring.prow.gardener.cloud>.'
      expr: |
        min(probe_success{instance="https://monitoring.prow.gardener.cloud"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: monitoring
    - alert: 'Site unavailable: https://storage.googleapis.com/gardener-prow/tide-history.json'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following site has been unhealthy (not 2xx HTTP response) for at least 2 minutes: <https://storage.googleapis.com/gardener-prow/tide-history.json|https://storage.googleapis.com/gardener-prow/tide-history.json>.'
      expr: |
        min(probe_success{instance="https://storage.googleapis.com/gardener-prow/tide-history.json"}) == 0
      for: 2m
      labels:
        severity: critical
  - name: Heartbeat ProwJobs
    rules:
    - alert: 'No recent successful heartbeat job runs'
      annotations:
        message: 'The heartbeat job `{{ $labels.job_name }}` has not had a successful run in the past 20m (should run every 5m).'
      expr: |
        sum(increase(prowjob_state_transitions{job_name=~"ci-infra-prow-checkconfig-(trusted|build)", state="success"}[20m])) by (job_name) < 0.5
      labels:
        severity: critical
        slo: prow-controller-manager
  - interval: 1m
    name: SLO Compliance
    rules:
    - expr: |
        min((absent(ALERTS{alertstate="firing", slo="deck"}) or absent(ALERTS{alertstate="firing", slo="hook"}) or absent(ALERTS{alertstate="firing", slo="prow-controller-manager"}) or absent(ALERTS{alertstate="firing", slo="sinker"}) or absent(ALERTS{alertstate="firing", slo="tide"}) or absent(ALERTS{alertstate="firing", slo="monitoring"})) or (ALERTS{alertstate="firing", slo=~"deck|hook|prow-controller-manager|sinker|tide|monitoring"} - 1)) without (alertstate)
      record: slo_component_ok
    - expr: (vector(1) unless min(slo_component_ok == 0)) or (slo_component_ok == 0)
      record: slo_prow_ok
  - name: prow
    rules:
    - alert: prow-pod-crashlooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container}}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
      expr: rate(kube_pod_container_status_restarts_total{namespace=~"prow|prow-monitoring",job="kube-state-metrics"}[5m]) * 60 * 5 > 0
      for: 1m
      labels:
        severity: critical
